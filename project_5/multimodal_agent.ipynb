{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thirumalakiran/buildingAgents/blob/main/project_5/multimodal_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4edbe397",
      "metadata": {
        "id": "4edbe397"
      },
      "source": [
        "# Project¬†5: **Build a Multi-Modal Generation Agent**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44fe0ead",
      "metadata": {
        "id": "44fe0ead"
      },
      "source": [
        "Welcome to the final project! In this project, you'll use open-source text-to-image and text-to-video models to generate content. Next, you'll build a **unified multi-modal agent** similar to modern chatbots, where a single agent can support general questions, image generation, and video generation requests.\n",
        "\n",
        "By the end of this project, you'll understand how to integrate multiple model types under one  routing system capable of deciding what modality to use based on the user's intent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3327e3b3",
      "metadata": {
        "id": "3327e3b3"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b842e61",
      "metadata": {
        "id": "8b842e61"
      },
      "source": [
        "* Use **Text-to-Image** models to generate images from a text.\n",
        "* Generate short clips with a **Text-to-Video** model\n",
        "* Build a **Multi-Modal Agent** that answers questions and routes media requests\n",
        "* Build a simple **Gradio** UI and interact with the multi-modal agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c433ac53",
      "metadata": {
        "id": "c433ac53"
      },
      "source": [
        "## Roadmap\n",
        "1. Environment setup\n",
        "2. Text‚Äëto‚ÄëImage\n",
        "3. Text‚Äëto‚ÄëVideo\n",
        "4. Multimodal Agent\n",
        "5. Gradio UI\n",
        "6. Celebrate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd9ad71d",
      "metadata": {
        "id": "fd9ad71d"
      },
      "source": [
        "## 1 - Environment Setup\n",
        "\n",
        "In this project, we'll use open-source Text-to-Image and Text-to-Video models to generate visuals from natural-language prompts. These models are computationally heavy and perform best on GPUs, so we recommend running this notebook in Google Colab or another GPU-enabled environment. We'll load all models from Hugging Face, which requires authentication.\n",
        "\n",
        "Before continuing:\n",
        "1. Open this project in Google Colab. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bytebyteai/ai-eng-projects/blob/main/project_5/multimodal_agent.ipynb)\n",
        "2. Create a Hugging Face account and generate an access token at huggingface.co/settings/tokens\n",
        "3. Paste your token in the field below to log in.\n",
        "4. In the Colab environment, enable GPU acceleration by selecting Runtime ‚Üí Change runtime type ‚Üí GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yNp2NUkF8p3m",
      "metadata": {
        "id": "yNp2NUkF8p3m"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"YOUR TOKEN HERE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6da7ce1",
      "metadata": {
        "id": "b6da7ce1"
      },
      "source": [
        "Let's import the required libraries and confirm that PyTorch can detect the available GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7950f70c",
      "metadata": {
        "id": "7950f70c"
      },
      "outputs": [],
      "source": [
        "import torch, diffusers, transformers, os, random, gc\n",
        "print('torch', torch.__version__, '| CUDA:', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "028f7c15",
      "metadata": {
        "id": "028f7c15"
      },
      "source": [
        "## 2 - Text-to-Image (T2I)\n",
        "T2I models translate natural-language descriptions into images. They are typically based on diffusion models, which gradually refine random noise into a coherent picture guided by the text prompt. In this section, you'll load and test one such model to generate images directly from text inputs.\n",
        "\n",
        "### 2.1: Load a T2I Model\n",
        "We'll use `Stable Diffusion XL` (SDXL) by `Stability AI`, one of the open-source diffusion models. It provides high-quality, detailed image generation with relatively efficient inference compared to earlier versions.\n",
        "\n",
        "You'll load the model from Hugging Face using the diffusers library, which simplifies running diffusion-based pipelines. To learn more about diffusers, read: https://huggingface.co/docs/diffusers/main/index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cfbedd4",
      "metadata": {
        "id": "8cfbedd4"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "# Define the Stable Diffusion XL model ID from Hugging Face and load the pre-trained model\n",
        "\"\"\"\n",
        "YOUR CODE HERE (~2-5 lines of code)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06d86e22",
      "metadata": {
        "id": "06d86e22"
      },
      "source": [
        "### 2.2: Generate an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7822b08",
      "metadata": {
        "id": "b7822b08"
      },
      "outputs": [],
      "source": [
        "# Generate and display an image from a text prompt using the loaded pipeline\n",
        "\"\"\"\n",
        "YOUR CODE HERE (~2 lines of code)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "540d2bb0",
      "metadata": {
        "id": "540d2bb0"
      },
      "source": [
        "### 2.3: Experimenting with \"inference_steps\"\n",
        "\n",
        "The number of inference steps determines how many refinement passes the diffusion model makes. Fewer steps give quicker but less detailed images, while more steps improve clarity and structure at the cost of speed.\n",
        "\n",
        "Try generating images with different step counts and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "139f30dd",
      "metadata": {
        "id": "139f30dd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate an image for different values of num_inference_steps (e.g., 10, 25, 50) and compare sharpness and detail\n",
        "images = []\n",
        "\n",
        "\"\"\"\n",
        "YOUR CODE HERE (~6-8 lines)\n",
        "\"\"\"\n",
        "\n",
        "# Plot results side-by-side\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, (steps, img) in enumerate(images, 1):\n",
        "    plt.subplot(1, len(images), i)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"{steps} steps\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa480aeb",
      "metadata": {
        "id": "aa480aeb"
      },
      "source": [
        "### 2.4 (Optional): Visualizing the Diffusion Process\n",
        "Diffusion models start from random noise and iteratively refine it into an image that matches the prompt. If you are curious, visualize all intermediate steps to see how the noise gradually turns into a coherent picture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147767b8",
      "metadata": {
        "id": "147767b8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Run the pipeline with 50 inference steps\n",
        "# Step 2: Capture intermediate latents or images during generation\n",
        "# Step 3: Plot them sequentially to show noise evolving into structure\n",
        "\"\"\"\n",
        "YOUR CODE HERE (~10-12 lines)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e5884c3",
      "metadata": {
        "id": "4e5884c3"
      },
      "source": [
        "### 2.5 (Optional): Experiment with other models.\n",
        "Different text-to-image models vary in speed, style, and visual quality. Try swapping in other open-source diffusion models and compare how their outputs differ in detail, realism, or artistic tone.\n",
        "\n",
        "You can browse available models on Hugging Face here: https://huggingface.co/models?library=diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6933ad43",
      "metadata": {
        "id": "6933ad43"
      },
      "outputs": [],
      "source": [
        "# Step 1: Replace model_id with another text-to-image model from Hugging Face\n",
        "# Step 2: Reload the pipeline and generate a few test images\n",
        "# Step 3: Compare image quality, color balance, and prompt fidelity\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090367b8",
      "metadata": {
        "id": "090367b8"
      },
      "source": [
        "## 3 - Text-to-Video (T2V)\n",
        "T2V models extend the idea of diffusion from still images to moving sequences. Instead of generating one frame, they create a series of coherent frames that depict motion consistent with the text prompt. These models are computationally heavier and often generate short clips (typically 2-10 seconds).\n",
        "\n",
        "In this section, you'll load an open-source video diffusion model and prepare it for generation.\n",
        "\n",
        "### 3.1: Load a T2V model\n",
        "\n",
        "We'll use the model `damo-vilab/text-to-video-ms-1.7b`, which can produce short video clips from text prompts. This model benefits from a specialized scheduler (DPMSolverMultistepScheduler) that improves stability and speed during sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e261bcd3",
      "metadata": {
        "id": "e261bcd3"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "\n",
        "video_model_id = 'damo-vilab/text-to-video-ms-1.7b'\n",
        "\n",
        "# Load the model with FP16 precision for efficiency\n",
        "\"\"\"\n",
        "YOUR CODE HERE (~2 lines of code)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "161fa323",
      "metadata": {
        "id": "161fa323"
      },
      "source": [
        "### 3.2: Generate a clip\n",
        "Create a short video clip from a text prompt using a text-to-video model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bXmM7Sa15bn9",
      "metadata": {
        "id": "bXmM7Sa15bn9"
      },
      "outputs": [],
      "source": [
        "# Step 1: Write a text prompt describing the video you want to generate\n",
        "# Step 2: Run the text-to-video pipeline with your chosen prompt\n",
        "\"\"\"\n",
        "YOUR CODE HERE (~2-3 lines)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fCOuEQDmaTU7",
      "metadata": {
        "id": "fCOuEQDmaTU7"
      },
      "source": [
        "### 3.3: Frame inspection\n",
        "Inspect a single frame to sanity-check colors, resolution, and subject positioning before writing a full video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MSvVHiG_7OpC",
      "metadata": {
        "id": "MSvVHiG_7OpC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Step 1: Select one frame from vid_frames (e.g., index 0)\n",
        "# Step 2: Convert float [0,1] frame to uint8 [0,255]\n",
        "# Step 3: Display as a PIL image\n",
        "\"\"\"\n",
        "YOUR CODE HERE (~1-3 lines)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56ea2781",
      "metadata": {
        "id": "56ea2781"
      },
      "source": [
        "### 3.4: Convert frames to MP4\n",
        "Write the generated frames to an MP4 file so you can preview and share the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FfS4oJJc52f9",
      "metadata": {
        "id": "FfS4oJJc52f9"
      },
      "outputs": [],
      "source": [
        "# Step 1: Use diffusers.utils.export_to_video to write vid_frames to an MP4\n",
        "# Step 2: Capture and print the saved video path\n",
        "\"\"\"\n",
        "YOUR CODE HERE (~3-4 lines)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nwn27N3rcZ2L",
      "metadata": {
        "id": "nwn27N3rcZ2L"
      },
      "source": [
        "### 3.5: Video inspection\n",
        "Play the saved video inside the notebook to check motion and temporal consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1PkAmKxHcUZy",
      "metadata": {
        "id": "1PkAmKxHcUZy"
      },
      "outputs": [],
      "source": [
        "# Display the saved MP4 inline\n",
        "from IPython.display import Video\n",
        "\n",
        "\"\"\"\n",
        "YOUR CODE HERE (1 line of code)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yVq2AgUKapTh",
      "metadata": {
        "id": "yVq2AgUKapTh"
      },
      "source": [
        "### 3.6 (Optional): Experiment with different configs\n",
        "Increase `num_frames` or decrease `num_inference_steps` to experiment with clip length versus quality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3aac4db",
      "metadata": {
        "id": "d3aac4db"
      },
      "source": [
        "## 4 - Multimodal Generation Agent\n",
        "Now that you have text-to-image, text-to-video, and basic LLM question answering, you will build a single agent that routes user requests to the right capability. The agent will read a prompt, infer intent (chat vs image vs video), and return the appropriate output.\n",
        "\n",
        "### 4.1: Load an LLM for generic queries\n",
        "Use a small LLM as the default chat brain. We will start with `gemma-3-1b-it` and keep the loading logic simple. You can swap to another compact chat model later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IZYNuZyiY5EZ",
      "metadata": {
        "id": "IZYNuZyiY5EZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch, textwrap, json, re\n",
        "\n",
        "# Load google/gemma-3-1b-it using Hugging Face\n",
        "\n",
        "\"\"\"\n",
        "YOUR CODE HERE (~2-15 lines)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vPU7s0zqeeg_",
      "metadata": {
        "id": "vPU7s0zqeeg_"
      },
      "source": [
        "### 4.2: Build a routing mechanism to route requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00767550",
      "metadata": {
        "id": "00767550"
      },
      "outputs": [],
      "source": [
        "def generate_media(prompt: str, mode: str):\n",
        "    # Produce either an image or a short video clip from a text prompt.\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE (~3-6 lines)\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def llm_generate(prompt, max_new_tokens=64, temperature=0.7):\n",
        "    # Return a response to the prompt with the loaded gemma\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE (~2 lines of code)\n",
        "    \"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HsJf8NeWdKIG",
      "metadata": {
        "id": "HsJf8NeWdKIG"
      },
      "outputs": [],
      "source": [
        "def classify_prompt(prompt: str):\n",
        "    \"\"\"Classify the user prompt into QA, image, or video.\"\"\"\n",
        "\n",
        "    # Step 1: Define a system prompt explaining how to classify requests (qa, image, video)\n",
        "    # Step 2: Format the user message and system message as input to the LLM\n",
        "    # Step 3: Generate a response with llm_generate() and parse it using regex\n",
        "    # Step 4: Extract fields \"type\" and \"expanded_prompt\" from the LLM response\n",
        "    # Step 5: Return a dict with classification results or default to {\"type\": \"qa\"} on failure\n",
        "\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE (~5-25 lines of code)\n",
        "    \"\"\"\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LiKCWnVaekar",
      "metadata": {
        "id": "LiKCWnVaekar"
      },
      "source": [
        "### 4.3: Build the multimodal agent\n",
        "This agent takes a single user prompt, sends it to the `classify_prompt` to determine what kind of task it is, and then calls the appropriate module:\n",
        "- QA: use the chat LLM to generate an answer\n",
        "- Image: use the text-to-image generator\n",
        "- Video: use the text-to-video generator\n",
        "\n",
        "Start with a simple version first. You can improve it later by adding better prompts, guardrails, and citation handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IW9eRebndgY2",
      "metadata": {
        "id": "IW9eRebndgY2"
      },
      "outputs": [],
      "source": [
        "def multimodal_agent(user_prompt: str):\n",
        "    # Step 1: Classify the request\n",
        "    # Step 2: Route the prompt and generate output\n",
        "\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE (~12-16 lines)\n",
        "    \"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NO3yNkY_eqec",
      "metadata": {
        "id": "NO3yNkY_eqec"
      },
      "source": [
        "### 4.4: Test the agent\n",
        "Now let's test your multimodal agent end to end. Each prompt will automatically be routed to the correct capability: text Q&A, image generation, or video generation, and display the corresponding output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff5f1ff",
      "metadata": {
        "id": "8ff5f1ff"
      },
      "outputs": [],
      "source": [
        "from diffusers.utils import export_to_video\n",
        "from IPython.display import display, Video\n",
        "\n",
        "# Step 1: Define a few diverse prompts (QA, image, video)\n",
        "# Step 2: For each prompt, call multimodal_agent and inspect the returned result\n",
        "\"\"\"\n",
        "YOUR CODE HERE (~15-18 lines)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zT5d0lnmeGvc",
      "metadata": {
        "id": "zT5d0lnmeGvc"
      },
      "source": [
        "Replace the sample queries with your own and verify that the agent chooses the correct generation path."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a218d0bb",
      "metadata": {
        "id": "a218d0bb"
      },
      "source": [
        "## 5 - Interactive Web UI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e16073ff",
      "metadata": {
        "id": "e16073ff"
      },
      "source": [
        "Launch a simple Gradio web interface so you (or your users) can play with the multimodal agent from the browser.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da883c7",
      "metadata": {
        "id": "4da883c7"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown('# Multimodal Agent')\n",
        "    inp = gr.Textbox(placeholder='Ask or create...')\n",
        "    btn = gr.Button('Submit')\n",
        "    out_text = gr.Markdown()\n",
        "    out_img = gr.Image()\n",
        "    out_vid = gr.Video()\n",
        "\n",
        "    def handle(prompt):\n",
        "        res = multimodal_agent(prompt)\n",
        "        if isinstance(res, str):\n",
        "            return res, None, None\n",
        "        elif hasattr(res, 'save'):\n",
        "            return '', res, None\n",
        "        else:\n",
        "            vid = export_to_video(res)\n",
        "            return '', None, vid\n",
        "\n",
        "    btn.click(handle, inp, [out_text, out_img, out_vid])\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e18ae1",
      "metadata": {
        "id": "37e18ae1"
      },
      "source": [
        "After the UI launches, open the link and generate your own images and videos directly from the browser."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "775d6b75",
      "metadata": {
        "id": "775d6b75"
      },
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "* You have built a **multi-modal agent** capable of understanding various requests, and routing them to the proper model.\n",
        "* Try experimenting with other T2I and T2V models.\n",
        "* Try making your system more efficient. For example, load a separate lightweight llm for routing, and a more capable llm for QA.\n",
        "\n",
        "\n",
        "üëè **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}